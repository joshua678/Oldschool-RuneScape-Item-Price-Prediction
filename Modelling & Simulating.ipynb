{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, pickle\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from typing import Dict, List\n",
    "\n",
    "print(\"PyTorch built with CUDA:\", torch.version.cuda)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"—\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = r\"./historical_1h_data\"\n",
    "OUTPUT_DIR = r\"./prediction_model\"\n",
    "\n",
    "SEQ_LEN = 24 # hours of past context\n",
    "STRIDE = 1 # hop size between windows\n",
    "PRED_HORIZON = 3 # hours ahead\n",
    "THRESH_PCT = 0.01 # +-1% band (since we are competing against a 1% sale tax)\n",
    "GRID_SWEEP = np.linspace(0.50, 0.95, 25) # candidate τ values\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 40\n",
    "LR = 1e-4\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "METRIC_BETA = 0.1\n",
    "\n",
    "IGNORE_IDX = -100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Data filter + feature engineering functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "START = pd.Timestamp(\"2025-04-01\")\n",
    "END = pd.Timestamp(\"2025-05-29 23:59:59\")\n",
    "\n",
    "def load_and_filter(start=START, end=END) -> List[pd.DataFrame]:\n",
    "    csv_files = glob.glob(os.path.join(INPUT_DIR, \"*.csv\"))\n",
    "    item_dfs = []\n",
    "\n",
    "    # build a complete hourly index and note how many hours that is\n",
    "    full_idx = pd.date_range(start, end, freq=\"H\")\n",
    "    total_expected = len(full_idx)\n",
    "\n",
    "    for fp in tqdm(csv_files, desc=\"Loading CSVs\"):\n",
    "        df = pd.read_csv(fp)\n",
    "        if \"datetime\" not in df.columns:\n",
    "            continue\n",
    "\n",
    "        # parse & limit to our window\n",
    "        df[\"datetime\"] = pd.to_datetime(df[\"datetime\"], errors=\"coerce\")\n",
    "        df = df.dropna(subset=[\"datetime\"])\n",
    "        df = df[df[\"datetime\"].between(start, end)].sort_values(\"datetime\")\n",
    "\n",
    "        # if too many missing raw prices, skip entirely\n",
    "        high_count = df[\"avgHighPrice\"].count()\n",
    "        low_count  = df[\"avgLowPrice\"].count()\n",
    "        if (high_count / total_expected < 0.9) or \\\n",
    "           (low_count / total_expected < 0.9):\n",
    "            continue\n",
    "\n",
    "        # re‐index to exact hourly grid\n",
    "        df = df.set_index(\"datetime\").reindex(full_idx)\n",
    "        df.index.name = \"datetime\"\n",
    "\n",
    "        # fill missing prices by carrying forward last known\n",
    "        df[\"avgHighPrice\"] = df[\"avgHighPrice\"].ffill()\n",
    "        df[\"avgLowPrice\"] = df[\"avgLowPrice\"].ffill()\n",
    "\n",
    "        # any originally‐missing volume becomes 0 (no trades that hour)\n",
    "        df[\"highPriceVolume\"] = df[\"highPriceVolume\"].fillna(0)\n",
    "        df[\"lowPriceVolume\"] = df[\"lowPriceVolume\"].fillna(0)\n",
    "\n",
    "        # fill item metadata across the new rows\n",
    "        df[\"item_name\"] = os.path.splitext(os.path.basename(fp))[0]\n",
    "        if \"item_id\" in df.columns:\n",
    "            df[\"item_id\"] = df[\"item_id\"].ffill().bfill()\n",
    "\n",
    "        # only keep items that now have exactly total_expected rows\n",
    "        if len(df) == total_expected:\n",
    "            item_dfs.append(df.reset_index())\n",
    "\n",
    "    print(f\"{len(item_dfs)} items kept, {len(item_dfs)*total_expected:,} rows.\")\n",
    "    return item_dfs\n",
    "\n",
    "\n",
    "def engineer_features(df: pd.DataFrame) -> Tuple[pd.DataFrame, list]:\n",
    "    # price & return features\n",
    "    # (we already zero‐filled volumes above, so these will be 0 where no trades)\n",
    "\n",
    "    df[\"mid\"] = (df[\"avgHighPrice\"] + df[\"avgLowPrice\"]) / 2\n",
    "    df[\"mid_return_1h\"] = np.log(df[\"mid\"]).diff(1)\n",
    "    df[\"mid_return_3h\"] = np.log(df[\"mid\"]).diff(3)\n",
    "    df[\"volatility_6h\"] = df[\"mid\"].rolling(6).std()\n",
    "    df[\"volatility_24h\"] = df[\"mid\"].rolling(24).std()\n",
    "    df[\"high_return_1h\"] = np.log(df[\"avgHighPrice\"]).diff(1)\n",
    "    df[\"low_return_1h\"] = np.log(df[\"avgLowPrice\"]).diff(1)\n",
    "    df[\"high_return_6h\"] = np.log(df[\"avgHighPrice\"]).diff(6)\n",
    "    df[\"low_return_6h\"] = np.log(df[\"avgLowPrice\"]).diff(6)\n",
    "    df[\"no_buys\"] = df[\"lowPriceVolume\"].fillna(0).eq(0)\n",
    "    df[\"no_sells\"] = df[\"highPriceVolume\"].fillna(0).eq(0)\n",
    "\n",
    "    # volume ratio (safe divide)\n",
    "    denom = df[\"highPriceVolume\"] + df[\"lowPriceVolume\"]\n",
    "    df[\"vol_ratio\"] = np.divide(\n",
    "        df[\"highPriceVolume\"], denom,\n",
    "        out=np.zeros_like(denom, dtype=float),\n",
    "        where=denom != 0,\n",
    "    )\n",
    "    df[\"low_volume_return_1h\"] = np.log(df[\"lowPriceVolume\"] + 1e-9).diff(1)\n",
    "    df[\"high_volume_return_1h\"] = np.log(df[\"highPriceVolume\"] + 1e-9).diff(1)\n",
    "\n",
    "    # cyclical time features\n",
    "\n",
    "    df[\"hour\"] = df[\"datetime\"].dt.hour\n",
    "    df[\"weekday\"] = df[\"datetime\"].dt.dayofweek\n",
    "    df[\"hour_sin\"] = np.sin(2*np.pi*df[\"hour\"]/24)\n",
    "    df[\"hour_cos\"] = np.cos(2*np.pi*df[\"hour\"]/24)\n",
    "    df[\"wday_sin\"] = np.sin(2*np.pi*df[\"weekday\"]/7)\n",
    "    df[\"wday_cos\"] = np.cos(2*np.pi*df[\"weekday\"]/7)\n",
    "    feat_cols = [\n",
    "        #\"avgHighPrice\", \"avgLowPrice\", \"mid\",\n",
    "        \"high_return_1h\", \"low_return_1h\", \"high_return_6h\", \"low_return_6h\",\n",
    "        \"vol_ratio\", \"low_volume_return_1h\", \"high_volume_return_1h\",\n",
    "        \"volatility_6h\", \"volatility_24h\",\n",
    "        #\"highPriceVolume\", \"lowPriceVolume\",\n",
    "        \"hour_sin\", \"hour_cos\", \"wday_sin\", \"wday_cos\",\n",
    "        \"no_buys\", \"no_sells\"\n",
    "    ]\n",
    "\n",
    "    # ensure numeric & finite; fill any leftover NaNs in the very first rows\n",
    "    df[feat_cols] = (\n",
    "        df[feat_cols]\n",
    "          .apply(pd.to_numeric, errors=\"coerce\")\n",
    "          .replace([np.inf, -np.inf], np.nan)\n",
    "          .ffill().bfill()\n",
    "    )\n",
    "\n",
    "    return df, feat_cols\n",
    "\n",
    "def make_labels(df: pd.DataFrame, horizon: int, thresh_pct: float\n",
    "                ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    high_next = df[\"avgHighPrice\"].shift(-horizon)\n",
    "    low_next  = df[\"avgLowPrice\"].shift(-horizon)\n",
    "\n",
    "    def label(curr, nxt):\n",
    "        pct = (nxt - curr) / curr\n",
    "        return np.where(pct >  thresh_pct, 2, # up\n",
    "               np.where(pct < -thresh_pct, 0, 1)).astype(np.int64)\n",
    "\n",
    "    y_high = label(df[\"avgHighPrice\"], high_next)\n",
    "    y_low  = label(df[\"avgLowPrice\"], low_next)\n",
    "    y_high[-horizon:] = -1\n",
    "    y_low [-horizon:] = -1\n",
    "    return y_high, y_low"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Torch dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PriceSequenceDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 arrays: List[np.ndarray],\n",
    "                 labels_high: List[np.ndarray],\n",
    "                 labels_low: List[np.ndarray],\n",
    "                 item_idxs: List[np.ndarray],\n",
    "                 seq_len: int,\n",
    "                 seq_stride: int = 1):\n",
    "        # 1) flatten all data\n",
    "        self.X = np.concatenate(arrays,   axis=0)\n",
    "        self.yH = np.concatenate(labels_high, axis=0)\n",
    "        self.yL = np.concatenate(labels_low,  axis=0)\n",
    "        self.item_i = np.concatenate(item_idxs,   axis=0)\n",
    "        self.seq_len = seq_len\n",
    "        self.stride = seq_stride\n",
    "\n",
    "        # 2) build per-item boundaries so windows never cross items\n",
    "        boundaries = []\n",
    "        offset = 0\n",
    "        for item_idx, arr in enumerate(arrays):\n",
    "            n = arr.shape[0]\n",
    "            boundaries.append((offset, offset + n, item_idx))\n",
    "            offset += n\n",
    "\n",
    "        # 3) collect only those idxs where a full seq_len history exists\n",
    "        valid_idx = []\n",
    "        for start, end, _ in boundaries:\n",
    "            # allow windows [i-seq_len, i) for i in [start+seq_len, end)\n",
    "            valid_idx.extend(range(start + seq_len, end, self.stride))\n",
    "        self.valid_idx = np.array(valid_idx, dtype=np.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_idx)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        idx = self.valid_idx[i]\n",
    "        x = self.X[idx-self.seq_len:idx]\n",
    "        iid = self.item_i[idx]\n",
    "        return (\n",
    "            torch.from_numpy(x).float(),\n",
    "            torch.tensor(self.yH[idx], dtype=torch.long),\n",
    "            torch.tensor(self.yL[idx], dtype=torch.long),\n",
    "            torch.tensor(iid, dtype=torch.long),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, n_features, n_items,\n",
    "                 id_emb_dim=1, hidden_size=24, dropout=0.3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.id_emb = nn.Embedding(n_items, id_emb_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=n_features + id_emb_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=2,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            bidirectional=False\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.head_high = nn.Linear(hidden_size*2, 3)\n",
    "        self.head_low = nn.Linear(hidden_size*2, 3)\n",
    "\n",
    "    def forward(self, x, item_id):\n",
    "        B, T, _ = x.shape\n",
    "        id_vec = self.id_emb(item_id).unsqueeze(1).expand(B, T, -1)\n",
    "        x = torch.cat([x, id_vec], dim=-1)\n",
    "        _, (h, _) = self.lstm(x)\n",
    "        h_top = self.dropout(torch.cat([h[-2], h[-1]], dim=1))  # fwd + bwd\n",
    "        return self.head_high(h_top), self.head_low(h_top)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Threshold search + metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def directional_metrics(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        beta=0.5,\n",
    "        mask=None,\n",
    "        ignore_idx=IGNORE_IDX,\n",
    "):\n",
    "    \"\"\"\n",
    "    Precision, recall and F-β for the directional classes 0 (rise) and 2 (fall).\n",
    "\n",
    "    Parameters:\n",
    "    y_true, y_pred : array-like of shape (n_samples,)\n",
    "    beta : float, weighting in F-score\n",
    "    mask : Boolean array, rows to keep   (optional)\n",
    "    ignore_idx : int, sentinel for 'skip row' (optional)\n",
    "\n",
    "    Returns:\n",
    "    precision, recall, f_beta  (all floats)\n",
    "    \"\"\"\n",
    "\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "\n",
    "    # base mask: keep only real labels 0 or 2\n",
    "    keep = np.isin(y_true, [0, 2])\n",
    "\n",
    "    # drop the sentinel if it somehow slipped through\n",
    "    keep &= (y_true != ignore_idx)\n",
    "\n",
    "    # apply mask if given\n",
    "    if mask is not None:\n",
    "        keep &= np.asarray(mask)\n",
    "\n",
    "    # nothing to score -> return zeros\n",
    "    if keep.sum() == 0:\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    p, r, f, _ = precision_recall_fscore_support(\n",
    "        y_true[keep],\n",
    "        y_pred[keep],\n",
    "        labels=[0, 2],\n",
    "        average=\"macro\",\n",
    "        beta=beta,\n",
    "        zero_division=0,\n",
    "    )\n",
    "    return p, r, f\n",
    "\n",
    "\n",
    "def tune_threshold(y_true,\n",
    "                   pred_raw,\n",
    "                   conf,\n",
    "                   sweep=np.linspace(0.50, 0.95, 25),\n",
    "                   beta=METRIC_BETA,\n",
    "                   head=\"H\"):\n",
    "    \"\"\"\n",
    "    Return the **highest** τ that achieves the best directional F-β,\n",
    "    and print the whole sweep so you can see what's happening.\n",
    "    \"\"\"\n",
    "    best_tau, best_f = None, -1.0\n",
    "\n",
    "    for τ in sweep:\n",
    "        pred = np.where(conf < τ, 1, pred_raw)\n",
    "        _, _, f = directional_metrics(y_true, pred, beta)\n",
    "\n",
    "        # keep the highest τ that attains the best F\n",
    "        if (f > best_f + 1e-6) or (abs(f - best_f) < 1e-6 and (best_tau is None or τ > best_tau)):\n",
    "            best_tau, best_f = τ, f\n",
    "    \n",
    "    return best_tau, best_f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Load & split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "item_dfs = load_and_filter()\n",
    "\n",
    "train_arrays, val_arrays, test_arrays = [], [], []\n",
    "train_H, val_H, test_H = [], [], []\n",
    "train_L, val_L, test_L = [], [], []\n",
    "train_idx, val_idx, test_idx = [], [], []\n",
    "\n",
    "for item_idx, df in enumerate(item_dfs):\n",
    "    df, feat_cols = engineer_features(df)\n",
    "\n",
    "    mask_train = (df[\"datetime\"] <  \"2025-05-16\")\n",
    "    mask_val = (df[\"datetime\"] >= \"2025-05-16\") & (df[\"datetime\"] < \"2025-05-23\")\n",
    "    mask_test = (df[\"datetime\"] >= \"2025-05-23\")\n",
    "\n",
    "    y_high, y_low = make_labels(df, PRED_HORIZON, THRESH_PCT)\n",
    "    y_high[df[\"no_buys\"].values] = IGNORE_IDX\n",
    "    y_low[df[\"no_sells\"].values] = IGNORE_IDX\n",
    "\n",
    "    def slice_it(mask, store_arrays, store_H, store_L, store_idx):\n",
    "        sub = df.loc[mask, feat_cols].values\n",
    "        store_arrays.append(sub)\n",
    "        m = mask.to_numpy()\n",
    "        store_H.append(y_high[m])\n",
    "        store_L.append(y_low[m])\n",
    "        # a 1D array of the same length as sub, filled with item_idx\n",
    "        store_idx.append(np.full(sub.shape[0], item_idx, dtype=np.int64))\n",
    "\n",
    "    slice_it(mask_train, train_arrays, train_H, train_L, train_idx)\n",
    "    slice_it(mask_val, val_arrays, val_H, val_L, val_idx)\n",
    "    slice_it(mask_test, test_arrays, test_H, test_L, test_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(np.concatenate(train_arrays, axis=0))\n",
    "scaler.scale_[scaler.scale_ == 0] = 1.0 # guard\n",
    "\n",
    "train_arrays = [scaler.transform(a) for a in train_arrays]\n",
    "val_arrays = [scaler.transform(a) for a in val_arrays]\n",
    "test_arrays = [scaler.transform(a) for a in test_arrays]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Torch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = PriceSequenceDataset(\n",
    "    train_arrays, train_H, train_L, train_idx,\n",
    "    seq_len=SEQ_LEN, seq_stride=STRIDE\n",
    ")\n",
    "ds_val = PriceSequenceDataset(\n",
    "    val_arrays, val_H, val_L, val_idx,\n",
    "    seq_len=SEQ_LEN, seq_stride=1\n",
    ")\n",
    "ds_test = PriceSequenceDataset(\n",
    "    test_arrays, test_H, test_L, test_idx,\n",
    "    seq_len=SEQ_LEN, seq_stride=1\n",
    ")\n",
    "\n",
    "print(f\"Train sequences: {len(ds_train):,} | Val: {len(ds_val):,} | Test: {len(ds_test):,}\")\n",
    "\n",
    "dl_train = DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True,  drop_last=True)\n",
    "dl_val = DataLoader(ds_val, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "dl_test = DataLoader(ds_test, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Class-weighted loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "yH_train_all = np.concatenate(train_H)\n",
    "yL_train_all = np.concatenate(train_L)\n",
    "N_CLASSES = 3\n",
    "\n",
    "def make_weights(y, n_classes=N_CLASSES, ignore_idx=IGNORE_IDX):\n",
    "    y = np.asarray(y)\n",
    "    y = y[y != ignore_idx] # keep only real labels\n",
    "    counts  = np.bincount(y, minlength=n_classes).astype(float)\n",
    "\n",
    "    # avoid divide-by-zero if a class is missing\n",
    "    counts[counts == 0] = 1.0\n",
    "\n",
    "    total   = counts.sum()\n",
    "    weights = total / (n_classes * counts)\n",
    "    return weights\n",
    "\n",
    "weight_high = torch.tensor(\n",
    "    make_weights(yH_train_all), dtype=torch.float32, device=DEVICE\n",
    ")\n",
    "weight_low  = torch.tensor(\n",
    "    make_weights(yL_train_all), dtype=torch.float32, device=DEVICE\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Class weights – HIGH:\", weight_high.cpu().numpy().round(2),\n",
    "        \"| LOW:\", weight_low.cpu().numpy().round(2))\n",
    "\n",
    "criterion_high = nn.CrossEntropyLoss(\n",
    "    weight=weight_high, ignore_index=IGNORE_IDX\n",
    ")\n",
    "criterion_low  = nn.CrossEntropyLoss(\n",
    "    weight=weight_low, ignore_index=IGNORE_IDX\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Training & saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMClassifier(n_features=len(feat_cols), n_items=len(item_dfs)).to(DEVICE)\n",
    "optimiser = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-3)\n",
    "#scheduler = ReduceLROnPlateau(\n",
    "#    optimiser,\n",
    "#    mode='max',\n",
    "#    factor=0.5,\n",
    "#    patience=10,\n",
    "#    threshold=5e-4,\n",
    "#    threshold_mode='abs',\n",
    "#    verbose=True,\n",
    "#    min_lr=5e-5\n",
    "#)\n",
    "\n",
    "history = defaultdict(list)\n",
    "best_val_dir_f05 = -1.0\n",
    "best_ckpt = None\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # training pass\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for X, yH, yL, item_id in dl_train:\n",
    "        X, yH, yL = X.to(DEVICE), yH.to(DEVICE), yL.to(DEVICE)\n",
    "        item_id = item_id.to(DEVICE)\n",
    "        optimiser.zero_grad()\n",
    "        outH, outL = model(X, item_id)\n",
    "        loss = criterion_high(outH, yH) + criterion_low(outL, yL)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimiser.step()\n",
    "        running_loss += loss.item() * X.size(0)\n",
    "\n",
    "    # validation pass\n",
    "\n",
    "    model.eval()\n",
    "    yH_true, yH_raw, pH_conf = [], [], []\n",
    "    yL_true, yL_raw, pL_conf = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, yH, yL, item_id in dl_val:\n",
    "            X       = X.to(DEVICE)\n",
    "            item_id = item_id.to(DEVICE)\n",
    "            outH, outL = model(X, item_id)\n",
    "\n",
    "            probsH = torch.softmax(outH, dim=1)\n",
    "            probsL = torch.softmax(outL, dim=1)\n",
    "\n",
    "            maxH, predH_raw = probsH.max(dim=1)\n",
    "            maxL, predL_raw = probsL.max(dim=1)\n",
    "\n",
    "            yH_true.extend(yH.numpy())\n",
    "            yL_true.extend(yL.numpy())\n",
    "            yH_raw.extend(predH_raw.cpu().numpy())\n",
    "            yL_raw.extend(predL_raw.cpu().numpy())\n",
    "            pH_conf.extend(maxH.cpu().numpy())\n",
    "            pL_conf.extend(maxL.cpu().numpy())\n",
    "\n",
    "    # tune τ per head\n",
    "\n",
    "    maskH = (np.array(yH_true) != IGNORE_IDX)\n",
    "    maskL = (np.array(yL_true) != IGNORE_IDX)\n",
    "\n",
    "    τH, _ = tune_threshold(\n",
    "        np.array(yH_true)[maskH],\n",
    "        np.array(yH_raw )[maskH],\n",
    "        np.array(pH_conf)[maskH],\n",
    "        head=\"H\",\n",
    "    )\n",
    "\n",
    "    τL, _ = tune_threshold(\n",
    "        np.array(yL_true)[maskL],\n",
    "        np.array(yL_raw )[maskL],\n",
    "        np.array(pL_conf)[maskL],\n",
    "        head=\"L\",\n",
    "    )\n",
    "\n",
    "    # apply tuned gate to compute final val predictions\n",
    "    yH_pred = np.where(np.array(pH_conf) < τH, 1, yH_raw)\n",
    "    yL_pred = np.where(np.array(pL_conf) < τL, 1, yL_raw)\n",
    "\n",
    "    pH, rH, fH = directional_metrics(yH_true, yH_pred, beta=METRIC_BETA, mask=maskH)\n",
    "    pL, rL, fL = directional_metrics(yL_true, yL_pred, beta=METRIC_BETA, mask=maskL)\n",
    "    dir_f05    = (fH + fL) / 2\n",
    "\n",
    "    # bookkeeping\n",
    "\n",
    "    history[\"loss\"].append(running_loss / len(ds_train))\n",
    "    history[\"prec_H\"].append(pH); history[\"prec_L\"].append(pL)\n",
    "    history[\"rec_H\"].append(rH); history[\"rec_L\"].append(rL)\n",
    "    history[\"f05_H\"].append(fH); history[\"f05_L\"].append(fL)\n",
    "    history[\"τH\"].append(τH); history[\"τL\"].append(τL)\n",
    "\n",
    "    current_lr = optimiser.param_groups[0]['lr']\n",
    "    print(f\"Epoch {epoch:02d} loss {history['loss'][-1]:.4f} | \"\n",
    "            f\"dir-F{METRIC_BETA} H {fH:.3f} L {fL:.3f} \"\n",
    "            f\"| prec H {pH:.3f} L {pL:.3f} | rec H {rH:.3f} L {rL:.3f} | LR: {current_lr} \"\n",
    "            f\"| τH {τH:.2f} τL {τL:.2f}\")\n",
    "    \n",
    "#    scheduler.step(dir_f05)\n",
    "\n",
    "    # early stop\n",
    "    \n",
    "    if dir_f05 > best_val_dir_f05:\n",
    "        best_val_dir_f05 = dir_f05\n",
    "        best_ckpt = {\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"thr_high\":  float(τH),\n",
    "            \"thr_low\":   float(τL),\n",
    "        }\n",
    "        torch.save(best_ckpt, Path(OUTPUT_DIR) / \"model.pt\")\n",
    "        print(f\"  ↳ new best model saved (↑ directional F-{METRIC_BETA})\")\n",
    "\n",
    "# Save scaler\n",
    "\n",
    "trained_items = [df[\"item_name\"].iloc[0] for df in item_dfs]\n",
    "\n",
    "with open(Path(OUTPUT_DIR) / \"scaler.pkl\", \"wb\") as fh:\n",
    "    pickle.dump(\n",
    "        {\n",
    "            \"mean\": scaler.mean_,\n",
    "            \"scale\": scaler.scale_,\n",
    "            \"feat_cols\": feat_cols,\n",
    "            \"seq_len\": SEQ_LEN,\n",
    "            \"trained_items\": trained_items\n",
    "        },\n",
    "        fh\n",
    "    )\n",
    "print(f\"\\nAll artefacts saved to {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(history[\"loss\"])\n",
    "plt.title(\"Train loss\")\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(history[\"prec_H\"], label=\"high\")\n",
    "plt.plot(history[\"prec_L\"], label=\"low\")\n",
    "plt.title(\"Directional precision\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(history[\"rec_H\"], label=\"high\")\n",
    "plt.plot(history[\"rec_L\"], label=\"low\")\n",
    "plt.title(\"Directional recall\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(history[\"f05_H\"], label=\"high\")\n",
    "plt.plot(history[\"f05_L\"], label=\"low\")\n",
    "plt.title(f\"Directional F-{METRIC_BETA}\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(Path(OUTPUT_DIR) / \"training_curves.png\", dpi=120)\n",
    "plt.show\n",
    "print(\"Saved training_curves.png inside the prediction_model folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Final test metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(Path(OUTPUT_DIR) / \"model.pt\", map_location=DEVICE)\n",
    "model.load_state_dict(ckpt[\"state_dict\"])\n",
    "τH, τL = ckpt[\"thr_high\"], ckpt[\"thr_low\"]\n",
    "model.eval()\n",
    "\n",
    "yH_true, yH_pred = [], []\n",
    "yL_true, yL_pred = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X, yH, yL, item_id in dl_test:\n",
    "        X = X.to(DEVICE)\n",
    "        item_id = item_id.to(DEVICE)\n",
    "        outH, outL = model(X, item_id)\n",
    "\n",
    "        probsH = torch.softmax(outH, dim=1)\n",
    "        probsL = torch.softmax(outL, dim=1)\n",
    "\n",
    "        maxH, predH_raw = probsH.max(dim=1)\n",
    "        maxL, predL_raw = probsL.max(dim=1)\n",
    "\n",
    "        predH = torch.where(maxH < τH, torch.tensor(1, device=DEVICE), predH_raw)\n",
    "        predL = torch.where(maxL < τL, torch.tensor(1, device=DEVICE), predL_raw)\n",
    "\n",
    "        yH_true.extend(yH.numpy()); yH_pred.extend(predH.cpu().numpy())\n",
    "        yL_true.extend(yL.numpy()); yL_pred.extend(predL.cpu().numpy())\n",
    "\n",
    "def report(head, y_true, y_pred):\n",
    "    labels = [0,1,2]\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    print(f\"\\nConfusion matrix – {head}\")\n",
    "    print(pd.DataFrame(cm, index=[f\"true_{c}\" for c in labels],\n",
    "                            columns=[f\"pred_{c}\" for c in labels]).to_string())\n",
    "\n",
    "    prec, rec, f1, supp = precision_recall_fscore_support(\n",
    "        y_true, y_pred, labels=labels, zero_division=0\n",
    "    )\n",
    "    df = pd.DataFrame(\n",
    "        {\"precision\": prec, \"recall\": rec, \"f1-score\": f1, \"support\": supp},\n",
    "        index=[f\"class_{c}\" for c in labels]\n",
    "    )\n",
    "    macro = pd.Series(\n",
    "        {\"precision\": prec.mean(), \"recall\": rec.mean(),\n",
    "            \"f1-score\": f1.mean(), \"support\": supp.sum()},\n",
    "        name=\"macro_avg\"\n",
    "    )\n",
    "    print(f\"\\nPrecision/Recall/F1 – {head}\")\n",
    "    print(pd.concat([df, macro.to_frame().T]).round(3).to_string())\n",
    "\n",
    "report(\"HIGH\", yH_true, yH_pred)\n",
    "report(\"LOW\" , yL_true, yL_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## Model Experimenting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_HOLD_H     = 48 # hours\n",
    "START_GP       = 100_000_000\n",
    "ALLOC_FRAC     = 0.33\n",
    "VOL_FRAC       = 0.30\n",
    "MAX_OPEN_POS   = 100\n",
    "\n",
    "START_TEST     = pd.Timestamp(\"2025-05-23\")\n",
    "END_TEST       = pd.Timestamp(\"2025-05-29 23:59:59\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def taxed_exit(gp_raw: float) -> float:\n",
    "    return gp_raw * 0.99\n",
    "\n",
    "\n",
    "def max_dd(equity: np.ndarray) -> float:\n",
    "    peak = equity[0]\n",
    "    mdd = 0.0\n",
    "    for v in equity:\n",
    "        peak = max(peak, v)\n",
    "        mdd = min(mdd, v / peak - 1)\n",
    "    return mdd\n",
    "\n",
    "def record_trade(trades, *, item, entry_t, exit_t, qty, cost_gp, exit_gp, forced=False):\n",
    "    \"\"\"Append a closed trade to the list\"\"\"\n",
    "    trades.append({\n",
    "        \"item\": item,\n",
    "        \"entry\": entry_t,\n",
    "        \"exit\": exit_t,\n",
    "        \"qty\": qty,\n",
    "        \"gp_in\": cost_gp,\n",
    "        \"gp_out\": exit_gp,\n",
    "        \"profit\": exit_gp - cost_gp,\n",
    "        \"forced\": forced,\n",
    "    })\n",
    "\n",
    "def dump_reports(trades, equity_curve, out_dir):\n",
    "    \"\"\"Win-rate, flips, per-item P&L, CSV files.\"\"\"\n",
    "    trades_df = pd.DataFrame(trades).sort_values(\"exit\")\n",
    "    flips = len(trades_df)\n",
    "    win_rate = (trades_df[\"profit\"] > 0).mean()\n",
    "    forced_count = trades_df[\"forced\"].sum()\n",
    "    holds = trades_df[\"exit\"] - trades_df[\"entry\"]\n",
    "    mean_hold = holds.mean()\n",
    "    mean_hours = mean_hold.total_seconds() / 3600\n",
    "    optional_count = len(trades_df) - forced_count\n",
    "\n",
    "    pnl_item = (\n",
    "        trades_df.groupby(\"item\")[\"profit\"]\n",
    "        .agg(trades=\"count\", total_gp=\"sum\")\n",
    "        .sort_values(\"total_gp\", ascending=False)\n",
    "    )\n",
    "\n",
    "    eq = np.asarray(equity_curve)\n",
    "    dd = max_dd(eq)\n",
    "\n",
    "    print(\"\\nRESULTS\")\n",
    "    print(f\"Final equity : {eq[-1]:,.0f} gp\")\n",
    "    print(f\"Return : {(eq[-1] / eq[0] - 1):.2%}\")\n",
    "    print(f\"Draw-down : {dd:.2%}\")\n",
    "    print(f\"Win-rate : {win_rate:.2%}\")\n",
    "    print(f\"Total flips : {flips}\")\n",
    "    print(f\"Forced closed: {forced_count}\")\n",
    "    print(f\"Mean length : {mean_hours:.2f} hours\")\n",
    "    print(\"\\nP&L BY ITEM\")\n",
    "    print(pnl_item.to_string())\n",
    "\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    trades_df.to_csv(out_dir / \"trades.csv\", index=False)\n",
    "    pnl_item.to_csv(out_dir / \"item_pnl.csv\")\n",
    "    print(f\"\\nCSV reports saved to {out_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_equity_curve(equity_curve: list[float],\n",
    "                      timestamps: list[pd.Timestamp]) -> None:\n",
    "    eq_ser = pd.Series(equity_curve,\n",
    "                       index=pd.DatetimeIndex(timestamps))\n",
    "\n",
    "    # drop the duplicates that cause vertical artefacts\n",
    "    eq_ser = eq_ser[~eq_ser.index.duplicated(keep=\"last\")]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(eq_ser.index, eq_ser.values, linewidth=1.2)\n",
    "    plt.title(\"Back-test Equity Curve\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Equity (gp)\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(Path(OUTPUT_DIR) / \"equity_curve.png\", dpi=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### Load artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "trades = []\n",
    "equity_curve = []\n",
    "equity_timestamps = []\n",
    "\n",
    "sd = pickle.load(open(Path(OUTPUT_DIR) / \"scaler.pkl\", \"rb\"))\n",
    "scaler = StandardScaler()\n",
    "scaler.mean_, scaler.scale_ = sd[\"mean\"], sd[\"scale\"]\n",
    "feat_cols   = sd[\"feat_cols\"]\n",
    "trained_items = [df[\"item_name\"].iloc[0] for df in item_dfs] #sd[\"trained_items\"]\n",
    "item2idx = {name: i for i, name in enumerate(trained_items)}\n",
    "\n",
    "ckpt = torch.load(Path(OUTPUT_DIR) / \"model.pt\", map_location=DEVICE)\n",
    "model = LSTMClassifier(n_features=len(feat_cols), n_items=len(trained_items)).to(DEVICE)\n",
    "model.load_state_dict(ckpt[\"state_dict\"])\n",
    "model.eval()\n",
    "TAU_HIGH = ckpt[\"thr_high\"]\n",
    "TAU_LOW  = ckpt[\"thr_low\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "### Load and prepare item data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rows      = [] # per-row dicts for global stream\n",
    "feats_store   = {} # item_name ➜ np.ndarray(features)\n",
    "last_mid      = {} # item_name ➜ live mid price (MTM)\n",
    "item_lengths  = {} # item_name ➜ len(df)\n",
    "\n",
    "# same hourly grid you used for training, but wider for context\n",
    "full_idx = pd.date_range(\n",
    "    START_TEST - pd.Timedelta(hours=SEQ_LEN),\n",
    "    END_TEST + pd.Timedelta(hours=PRED_HORIZON),\n",
    "    freq=\"H\"\n",
    ")\n",
    "total_expected = len(full_idx)\n",
    "\n",
    "for fp in glob.glob(os.path.join(INPUT_DIR, \"*.csv\")):\n",
    "    name = Path(fp).stem\n",
    "    if name not in trained_items:\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(fp)\n",
    "    if \"datetime\" not in df.columns:\n",
    "        continue\n",
    "\n",
    "    # basic cleaning\n",
    "\n",
    "    df[\"datetime\"] = pd.to_datetime(df[\"datetime\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"datetime\"])\n",
    "    df = df.set_index(\"datetime\")\n",
    "\n",
    "    # keep only the wide test window\n",
    "    df = df[df.index.to_series().between(full_idx[0], full_idx[-1])]\n",
    "    if df.empty:\n",
    "        continue\n",
    "\n",
    "    # completeness check\n",
    "\n",
    "    #high_cnt = df[\"avgHighPrice\"].count()\n",
    "    #low_cnt  = df[\"avgLowPrice\"].count()\n",
    "    #if (high_cnt / total_expected < 0.9) or (low_cnt / total_expected < 0.9):\n",
    "        # skip items that were rejected in training\n",
    "    #    continue\n",
    "\n",
    "    # re-index & impute\n",
    "    df = df.reindex(full_idx) # exact hourly grid\n",
    "    df.index.name = \"datetime\"\n",
    "\n",
    "    # forward-fill prices\n",
    "    df[\"avgHighPrice\"] = df[\"avgHighPrice\"].ffill()\n",
    "    df[\"avgLowPrice\"] = df[\"avgLowPrice\"].ffill()\n",
    "\n",
    "    # zero-fill volumes (no trades that hour)\n",
    "    df[\"highPriceVolume\"] = df[\"highPriceVolume\"].fillna(0)\n",
    "    df[\"lowPriceVolume\"] = df[\"lowPriceVolume\"].fillna(0)\n",
    "\n",
    "    # carry metadata forward/backward\n",
    "    df[\"item_name\"] = name\n",
    "    if \"item_id\" in df.columns:\n",
    "        df[\"item_id\"] = df[\"item_id\"].ffill().bfill()\n",
    "\n",
    "    # final length check (should always pass now)\n",
    "    if len(df) != total_expected:\n",
    "        continue\n",
    "\n",
    "    # feature engineering & storage\n",
    "\n",
    "    df = df.reset_index() # back to column “datetime”\n",
    "    df, feat_cols = engineer_features(df)\n",
    "    feats_store[name] = scaler.transform(\n",
    "        df[feat_cols].values.astype(np.float32)\n",
    "    )\n",
    "\n",
    "    item_lengths[name] = len(df)\n",
    "    last_mid[name] = np.nan # filled online\n",
    "\n",
    "    # rows for the global, time-ordered stream\n",
    "    rows = df[[\"datetime\", \"item_name\", \"mid\", \"highPriceVolume\"]].copy()\n",
    "    rows[\"row_idx\"] = np.arange(len(df))\n",
    "    rows[\"item_idx\"] = item2idx[name]\n",
    "    all_rows.append(rows)\n",
    "\n",
    "# global stream sorted by wall-clock time\n",
    "stream = (\n",
    "    pd.concat(all_rows)\n",
    "      .sort_values(\"datetime\")\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(f\"Loaded {stream['item_name'].nunique()} items, {len(stream):,} rows in event stream.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "### Backtest loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "cash = START_GP\n",
    "open_pos: Dict[str, dict] = {}\n",
    "equity_curve.append(cash)\n",
    "equity_timestamps.append(START_TEST)\n",
    "pbar = tqdm(total=len(stream), desc=\"Simulating\")\n",
    "\n",
    "for row in stream.itertuples(index=False):\n",
    "    pbar.update(1)\n",
    "    t = row.datetime\n",
    "    if START_TEST <= t <= END_TEST:\n",
    "        mtm = sum(\n",
    "            pos[\"qty\"] * last_mid[name]\n",
    "            for name, pos in open_pos.items()\n",
    "            if not np.isnan(last_mid[name])\n",
    "        )\n",
    "        equity_curve.append(cash + mtm)\n",
    "        equity_timestamps.append(t)\n",
    "\n",
    "    item_name = row.item_name\n",
    "    item_idx = row.item_idx\n",
    "    idx_in_item = row.row_idx\n",
    "    mid_price = row.mid\n",
    "    vol_hour = row.highPriceVolume\n",
    "\n",
    "    # update last seen price for this item\n",
    "    if not np.isnan(mid_price):\n",
    "        last_mid[item_name] = mid_price\n",
    "\n",
    "    # skip until in live test window and enough history for a prediction\n",
    "    if not (START_TEST <= t <= END_TEST):\n",
    "        continue\n",
    "    if idx_in_item < SEQ_LEN or idx_in_item + PRED_HORIZON >= item_lengths[item_name]:\n",
    "        continue\n",
    "\n",
    "    # model forecast\n",
    "\n",
    "    X_np = feats_store[item_name][idx_in_item - SEQ_LEN : idx_in_item]\n",
    "    if np.isnan(X_np).any():\n",
    "        continue\n",
    "    X = torch.from_numpy(X_np).unsqueeze(0).to(DEVICE)  # (1, T, F)\n",
    "    with torch.no_grad():\n",
    "        outH, outL = model(X, torch.tensor([item_idx], device=DEVICE))\n",
    "        pH = torch.softmax(outH, 1)[0]\n",
    "        pL = torch.softmax(outL, 1)[0]\n",
    "    predH, confH = int(pH.argmax()), float(pH.max())\n",
    "    predL, confL = int(pL.argmax()), float(pL.max())\n",
    "\n",
    "    bullish = (\n",
    "        predH == 2 and predL == 2 and\n",
    "        confH >= TAU_HIGH and confL >= TAU_LOW\n",
    "    )\n",
    "    bearish = (\n",
    "        predH == 0 and predL == 0 and\n",
    "        confH >= TAU_HIGH and confL >= TAU_LOW\n",
    "    )\n",
    "\n",
    "    # manage open position\n",
    "\n",
    "    if item_name in open_pos:\n",
    "        pos = open_pos[item_name]\n",
    "        held_hours = (t - pos[\"entry_time\"]).total_seconds() / 3600.0\n",
    "        if bearish or held_hours >= MAX_HOLD_H:\n",
    "            exit_gp_raw = pos[\"qty\"] * mid_price\n",
    "            proceeds = taxed_exit(pos[\"qty\"] * mid_price)\n",
    "            record_trade(\n",
    "                trades,\n",
    "                item=item_name,\n",
    "                entry_t=pos[\"entry_time\"],\n",
    "                exit_t=t,\n",
    "                qty=pos[\"qty\"],\n",
    "                cost_gp=pos[\"cost\"],\n",
    "                exit_gp=proceeds,\n",
    "                forced = (held_hours >= MAX_HOLD_H)\n",
    "            )\n",
    "            cash += proceeds\n",
    "            del open_pos[item_name]\n",
    "\n",
    "    # open new position\n",
    "\n",
    "    if bullish and item_name not in open_pos and len(open_pos) < MAX_OPEN_POS:\n",
    "        if np.isnan(mid_price) or mid_price == 0:\n",
    "            pass # skip if price bad\n",
    "        else:\n",
    "            volume_cap = int(vol_hour * VOL_FRAC)\n",
    "            max_qty_cash = int((cash * ALLOC_FRAC) // mid_price)\n",
    "            qty = min(volume_cap, max_qty_cash)\n",
    "            if qty > 0:\n",
    "                total_cost = qty * mid_price\n",
    "                cash -= total_cost\n",
    "                open_pos[item_name] = {\n",
    "                    \"entry_time\": t,\n",
    "                    \"idx\": idx_in_item,\n",
    "                    \"qty\": qty,\n",
    "                    \"cost\": total_cost,\n",
    "                    \"closed\": False,\n",
    "                }\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "# Force close remaining positions\n",
    "\n",
    "for name, pos in list(open_pos.items()):\n",
    "    last_px = last_mid.get(name, np.nan)\n",
    "    if np.isnan(last_px):\n",
    "        continue\n",
    "    proceeds = taxed_exit(pos[\"qty\"] * last_px)\n",
    "    cash    += proceeds\n",
    "    pos[\"exit_time\"] = END_TEST\n",
    "    pos[\"gp_out\"] = proceeds\n",
    "    pos[\"profit\"] = proceeds - pos[\"cost\"]\n",
    "    pos[\"closed\"] = True\n",
    "    #equity_curve.append(cash)\n",
    "    #equity_timestamps.append(t)\n",
    "    del open_pos[name]\n",
    "\n",
    "equity_curve.append(cash)\n",
    "equity_timestamps.append(END_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "### Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "equity = np.array(equity_curve)\n",
    "\n",
    "# build trades dataframe from positions we closed\n",
    "closed_positions = [pos for pos in last_mid.keys()  # placeholder\n",
    "                    if False]  # TODO – retained for possible extensions\n",
    "\n",
    "out_dir = Path(OUTPUT_DIR)\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "dump_reports(trades, equity_curve, Path(OUTPUT_DIR))\n",
    "plot_equity_curve(equity_curve, equity_timestamps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
